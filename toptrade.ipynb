{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of toptrade.ipynb","provenance":[{"file_id":"1Mdd7NA75NT3P8NaCTbDPcyCmKRrqQBvc","timestamp":1578437175095}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"HwzDza1-g2Oj","colab_type":"code","colab":{}},"source":["#Install tensorflow GPU and other general project dependencies\n","%tensorflow_version 2.x\n","%pip install yfinance\n","%pip install quandl\n","import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K0oL0Su7bOrk","colab_type":"code","colab":{}},"source":["#Install and configure Tensorboard dependencies for Google Colab\n","!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip ngrok-stable-linux-amd64.zip\n","\n","\n","LOG_DIR = './logs'\n","get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","    .format(LOG_DIR)\n",")\n","\n","get_ipython().system_raw('./ngrok http 6006 &')\n","\n","!curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FvdhT47LxDN1","colab_type":"code","colab":{}},"source":["#Download ccryptocurrency historical data\n","!wget https://pythonprogramming.net/static/downloads/machine-learning-data/crypto_data.zip\n","!unzip crypto_data.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E_bXjdn5MwHE","colab_type":"code","colab":{}},"source":["import time\n","import quandl\n","import random \n","import numpy as np\n","import pandas as pd\n","import yfinance as yf\n","from datetime import datetime \n","from collections import deque\n","from sklearn import preprocessing\n","from sklearn.feature_selection import chi2\n","from sklearn.feature_selection import SelectKBest\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.callbacks import TensorBoard\n","from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G2qFZ8kcMxgx","colab_type":"code","colab":{}},"source":["#Global configuration variables\n","RATIOS = [\"BTC-USD\", \"LTC-USD\", \"BCH-USD\", \"ETH-USD\"]\n","TARGET_RATIO = \"ETH-USD\"\n","RATIO_LABELS = [\"time\", \"low\", \"high\", \"open\", \"close\", \"volume\"]\n","FUTURE_PERIOD_PREDICT = 3\n","SEQ_LEN = 60\n","EPOCHS = 10\n","BATCH_SIZE = 64\n","VALIDATION_PERCENTAGE_SIZE = .05\n","LSTM_MODEL_PARAMETERS = {\n","    \"nodes_per_LSTM_layer\": [128, 256], \n","    \"nodes_per_dense_layer\": [32, 64],\n","    \"num_of_LSTM_layers\": [2, 3], #Assumes at minimum 2 LSTM layers\n","    \"num_of_dense_layers\": [1, 2]\n","}\n","NAME = f\"{TARGET_RATIO}-LSTM-\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bl2ACdOBmFHk","colab_type":"code","colab":{}},"source":["def get_data():\n","  #Inialize empty main df \n","  df = pd.DataFrame() \n","\n","  #Iteratively add Close and Volume columns of ratios to df \n","  for ratio in RATIOS:  \n","    ratio_path_string = f\"crypto_data/{ratio}.csv\"\n","    ratio_df = pd.read_csv(ratio_path_string, names=RATIO_LABELS)  \n","    ratio_df.rename(columns={\"close\": f\"{ratio}_Close\", \"volume\": f\"{ratio}_Volume\"}, inplace=True)\n","    ratio_df.set_index(\"time\", inplace=True) \n","    ratio_df = ratio_df[[f\"{ratio}_Close\", f\"{ratio}_Volume\"]]  \n","    if len(df)==0:  \n","      df = ratio_df  \n","    else:  \n","      df = df.join(ratio_df)\n","\n","  # Add technical features corresponding to TARGET_RATIO'S pricing movement \n","  df = get_technical_features(df, TARGET_RATIO)\n","  df.fillna(method=\"ffill\", inplace=True)  \n","  df.dropna(inplace=True)\n","  \n","  return df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TnQJ-aphhkYl","colab_type":"code","colab":{}},"source":["def get_technical_features(df, ratio):\n","    # Create 7 and 21 days Moving Average\n","    df[ratio + \"_MA7\"] = df[ratio + \"_Close\"].rolling(window=7).mean()\n","    df[ratio + \"_MA21\"] = df[ratio + \"_Close\"].rolling(window=21).mean()\n","    \n","    # Create MACD\n","    df[ratio + \"_26EMA\"] = df[ratio + \"_Close\"].ewm(span=26).mean()\n","    df[ratio + \"_12EMA\"] = df[ratio + \"_Close\"].ewm(span=12).mean()\n","    df[ratio + \"_MACD\"] = (df[ratio + \"_12EMA\"] - df[ratio + \"_26EMA\"])\n","    \n","    # Create Bollinger Bands\n","    df[ratio + \"_20SD\"] = df[ratio + \"_Close\"].rolling(20).std()\n","    df[ratio + \"_Upper_Band\"] = df[ratio + \"_MA21\"] + (df[ratio + \"_20SD\"]*2)\n","    df[ratio+ \"_Lower_Band\"] = df[ratio + \"_MA21\"] - (df[ratio + \"_20SD\"]*2)\n","    \n","    # Create Exponential moving average\n","    df[ratio + \"_EMA\"] = df[ratio + \"_Close\"].ewm(com=0.5).mean()\n","    \n","    return df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"98-YRtD7hkif","colab_type":"code","colab":{}},"source":["def classify(current, future):\n","  if float(future) > float(current):\n","    return 1\n","  else:\n","    return 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BqrHeB9vhkry","colab_type":"code","colab":{}},"source":["def get_target(df): \n","  df['Future'] = df[TARGET_RATIO + \"_Close\"].shift(-FUTURE_PERIOD_PREDICT)\n","  df['Target'] = list(map(classify, df[TARGET_RATIO + \"_Close\"], df['Future']))\n","  return df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"08eaK8smMxxG","colab_type":"code","colab":{}},"source":["def preprocess(df):\n","  #Normalize data by converting to percentage change\n","  df = df.drop(\"Future\", 1)\n","  for col in df.columns:\n","      if col != \"Target\":\n","        df[col] = df[col].pct_change()\n","        df.dropna(inplace=True)\n","        df[col] = preprocessing.scale(df[col].values)\n","    \n","  df.dropna(inplace=True)\n","    \n","  #List of sequences \n","  sequential_data = []\n","  #Sequence with fixed length of SEQ_LEN\n","  prev_days = deque(maxlen=SEQ_LEN)\n","    \n","  for i in df.values:\n","    prev_days.append([n for n in i[:-1]])\n","    if len(prev_days) == SEQ_LEN:\n","      sequential_data.append([np.array(prev_days), i[-1]])\n","        \n","  random.shuffle(sequential_data)\n","    \n","  #Lists to sore buy/sell sequences and targets\n","  buys = []\n","  sells = []\n","    \n","  #Partition data into two seperate list buys and selss\n","  for sequence, target in sequential_data: \n","    if target == 1:\n","      buys.append([sequence, target])\n","    else:\n","      sells.append([sequence, target])\n","    \n","  random.shuffle(buys)  \n","  random.shuffle(sells)\n","    \n","  lower_class_count = min(len(buys), len(sells))\n","    \n","  #Downsample majority class \n","  buys = buys[:lower_class_count]\n","  sells = sells[:lower_class_count]\n","    \n","  sequential_data = buys+sells\n","  random.shuffle(sequential_data)\n","    \n","  X = []\n","  y = []\n","    \n","  #Seperate data into feature list X and target list y\n","  for seq, target in sequential_data:  \n","    X.append(seq)  \n","    y.append(target)  \n","\n","  return np.array(X), np.array(y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lukRgSSrMx5m","colab_type":"code","colab":{}},"source":["def train_validation_split(df):\n","  split = sorted(df.index.values)[-int(VALIDATION_PERCENTAGE_SIZE*df.shape[0])]\n","  validation_df = df[(df.index >= split)]\n","  df = df[(df.index < split)]\n","  return df, validation_df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TY8EqZk-hwh3","colab_type":"code","colab":{}},"source":["def build_dataset():\n","  data = get_data()\n","  df = get_target(data)\n","  df, validation_df = train_validation_split(df)\n","  train_x, train_y = preprocess(df)\n","  test_x, test_y = preprocess(validation_df)\n","  return train_x, train_y, test_x, test_y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BZx2kmJHlbzt","colab_type":"code","colab":{}},"source":["def generate_models(): \n","  train_x, train_y, test_x, test_y = build_dataset()\n","  for nodes_per_LSTM_layer in LSTM_MODEL_PARAMETERS[\"nodes_per_LSTM_layer\"]: \n","    for nodes_per_dense_layer in LSTM_MODEL_PARAMETERS[\"nodes_per_dense_layer\"]: \n","      for num_of_LSTM_layers in LSTM_MODEL_PARAMETERS[\"num_of_LSTM_layers\"]: \n","        for num_of_dense_layers in LSTM_MODEL_PARAMETERS[\"num_of_dense_layers\"]: \n","          model = Sequential()\n","\n","          #Add first requried LSTM layer\n","          model.add(LSTM(nodes_per_LSTM_layer, input_shape=(train_x.shape[1:]), return_sequences=True))\n","          model.add(Dropout(0.2))\n","          model.add(BatchNormalization())  #normalizes activation outputs\n","\n","          #Iteratively add additional LSTM layers based on model parameters \n","          for i in range(num_of_LSTM_layers - 1): \n","            return_sequence = False if (i == num_of_LSTM_layers - 2) else True \n","            model.add(LSTM(nodes_per_LSTM_layer, return_sequences=return_sequence))\n","            model.add(Dropout(0.2))\n","            model.add(BatchNormalization())  #normalizes activation outputs\n","\n","          #Iteratively add additional dense layers based on model parameters \n","          for i in range(num_of_dense_layers): \n","            model.add(Dense(nodes_per_dense_layer, activation='relu'))\n","            model.add(Dropout(0.2))\n","\n","          #Add final dense layer of size 2 and softmax activation to faciliate classification \n","          model.add(Dense(2, activation='softmax'))\n","\n","          opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n","\n","          currrent_model_string = f\"{NAME}-{str(nodes_per_LSTM_layer)}-{str(nodes_per_dense_layer)}-{str(num_of_LSTM_layers)}-{str(num_of_dense_layers)}\"\n","          tensorboard = TensorBoard(log_dir=\"logs/{}\".format(currrent_model_string), histogram_freq=1, write_graph=True, write_grads=True, batch_size=BATCH_SIZE, write_images=True)\n","\n","          model.compile(\n","            loss='sparse_categorical_crossentropy',\n","            optimizer=opt,\n","            metrics=['accuracy']\n","          )\n","\n","          # Train model\n","          history = model.fit(\n","            train_x, train_y,\n","            batch_size=BATCH_SIZE,\n","            epochs=EPOCHS,\n","            validation_data=(test_x, test_y),\n","            callbacks=[tensorboard]\n","          )\n","\n","          # Score model\n","          score = model.evaluate(test_x, test_y, verbose=0)\n","          print('Test loss:', score[0])\n","          print('Test accuracy:', score[1])\n","\n","          # Save model\n","          model.save(\"models/{}\".format(currrent_model_string))      \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sCKn3POFi7fv","colab_type":"code","colab":{}},"source":["#Run function to iniate trainign and evaluating models\n","generate_models()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s3APqbPaSVul","colab_type":"text"},"source":[""]}]}