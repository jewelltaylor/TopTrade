# TopTrade

TopTrade is a machine leaning system that aims to predict whether a certain cryptocurrency is a buy or a sell over a specified trading period. For the purpose of this project, the cryptocurrencies that I will be focusing on are Bitcoin, Ethereum, Litecoin and Bitcoin Cash because they are among the most prominent cryptocurrencies. This project includes data wrangling, data visalization, feature engineering, preprocessing data, generating models and hyperparameter tuning. The following document will outline the high level approaches I have taken to develop this project. 

## Introduction
### Data
The data in this project was sourced from the Poloniex API. The Poloniex API gives access to data from the most popular cryptocurrencies, including the ones that I will be using, at a variety of granularities. For each of the four cryptocurrencies we are interested in the close and the volume values at the minute level. The close is the cryptocurrencies market price at the end of the minute interval and the volume is the amount of units that were exchanged during the minute interval. Below is initial structure of the data as described above: 

<img width="672" alt="Screen Shot 2020-01-07 at 10 10 23 PM" src="https://user-images.githubusercontent.com/34798787/71947517-1f1e6900-319b-11ea-960d-d607dd76753b.png">

### Features 
With the goal of flexibilty in mind, I would like the ability to dynamically change the cryptocurrency that will be predicted. As a result, the preliminary set of features will consist of the historical values of the volume of the cryptocurrency being predicted along with the historical values of the volume and close of the remaining three cryptocurrencies. It is my hope that this set of features will offer insight into the price movement of the target cryptocurrency. The high level hypothesis I am making is that the prices and volumes of the various cryptocurrencies are related in some way. In order to test this assumption, I looked into how the prices of the various cryptocurrencies are correlated. This was done by taking the original dataset and selecting only columns corresponding to cryptocurrencies prices (4 in total). I then normalized this data by taking the percentage change between subsequent values in each column. The pearson correlation coefficitents were then calculated between the prices of cryptocurrencu. An illustrustion of the aforementioned analysis is below: 

<img width="368" alt="Screen Shot 2020-01-07 at 5 40 13 PM" src="https://user-images.githubusercontent.com/34798787/71948527-47f42d80-319e-11ea-82f5-9a3725bb2551.png">

As evidenced by the values of the pearson correlation coeficients exceeding .5, there is some degree of relationship between the price movement of the various cryptocurrencies. This relationship can potentially be exploited by a model to predict whether a cryptocurrency is a buy or a sell over a specified trading period. 

### Feature Engineering
Based on the existing features, I will derive additional potentially insightful features. This process is known as feature engineering. In particular, I will be creating features based on the price movement of the target cryptocurrency referred to as technical indicators. Technical indicators are a mathematical calculation based on historic price, or other related information, that aims to forecast the direction of the price of an asset. Below is a list of technical indicators that were derived: 

- **7 Interval Moving Average:** The average of the prices over the last 7 intervals.
- **21 Interval Moving Average:** The average of the prices over the last 21 intervals.
- **12 Interval Exponential Moving Average:** The average of the prices over the last 12 intervals with more wighting to more recent prices. 
- **26 Interval Exponential Moving Average:** The average of the prices over the last 26 intervals with more weighting to more recent prices. 
- **20 Interval Standard Deviation:** The standard deviation of the 20 previous intervals. 
- **Bollinger Bands:** A Bollinger Band is a technical analysis tool defined by a set of lines plotted two standard deviations (positively and negatively) away from a simple moving average (SMA) of the price

### Preprocessing 
Now that I have generated a set of meaningful features (X), I will preprocess the data in order to prepare it to be fed into the subsequent model. First, The target column (y) is generated. This is done in two steps. Initially, the Future column is generate done by shifting price of the target cryptocurrency in the future a specified amount of intervals. By doing so, the Future column represents the price of the target cryptocurrncy a certain amount of periods in the future. The amount of periods in the future to predict can be set dynamically. I have a chosen a seemingly reasnable value of 3. Based off the Future Column, the "y" column is generated by comparing the current price of the target cruptocurrency to the Future column. If the future price is higher then the current price, the y value of the observation is buy (1), otherwise the y values of the observation is a sell (0). The resulting dataset is shown below: 

<img width="559" alt="Screen Shot 2020-01-07 at 11 53 59 PM" src="https://user-images.githubusercontent.com/34798787/71951426-03ba5a80-31a9-11ea-9f2d-aa33cb9cdd70.png">

Following the removal of the Future column, the data is normalized by converting values to percent change over time and scaling the resulting values from 0 to 1 for each column in the dataset except the y column. The y column is also removed from the dataset and stored in its own seperate sequence. Next, the feature dataset is generating by transforming the existing dataset into sequences of observations of a specified length. This length can be set dynamically. I have chosen a seemingly reasonable length of 60. As such, the feature dataset is transformed into a three dimensional matrix where each observation corresponds to sequences of 60 records with 17 columns that represent the features of each record. Accordingly, we have our feature dataset and corresponding target sequence that we will divide into traing and testing sets. As the names imply, the training set will be used to train the model and the testing sets will be used to test the model. 

## Model 
### Model Information
When selecting the model to generate the predictions, it is important to consider the characteristics of the data at hand. It is clear the data is functional in the sense that it takes place over the dimension of time. A specific type of deep learning models called Recurrent Neural Networks (RNN) are particularly well suited for forecasting problems of this nature. An RNN is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs. This makes them applicable to tasks involving functional data including asset price forecasting. In particular, we be using a Long Short-Term Memory (LSTM), a specific type of RNN. A LSTM Model consists of several LSTM units that facilitate the flow of data through the model. A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell. Below is an illustration of a LSTM unit: 

![The_LSTM_cell](https://user-images.githubusercontent.com/34798787/71954598-cdcea380-31b3-11ea-93ab-af83f74e3a25.png)

### Hyperparameter Tuning 
The LSTM model has a number of hyperparameters that dictate the architecture and other high level information about the model. In order to generate the best model possible, I will try different permutations of the hyperparameters and select the model with the best performance. Due to the intensive nature of the computation involved with the corresponding grid search for the model's hyperparameters, only a limited number of permutations of model hyperparameters will be evaluated. Below are the three hyperparameters that will be involved in this grid search: 
- **LSTM Layer Units:** The number of nodes in each LSTM layer.
- **Dense Layer Units:** The number of nodes in each dense layer. 
- **LSTM Layers:** The number of LSTM layers in the model. 

### Training and Evaluation 
The model described in the previous section is trained on a set of 80000 observations. The loss metric used in the process of training the model is sparse categorical cross entropy. Sparse categorical cross entropy is a loss function that measures the difference between the distribution of observed class labels and the predicted probabilities of class membership. The optimizer used in the process of training the model is the Adam optimizer. The Adam optimizer is an adaptive learning rate optimization algorithm thatâ€™s been designed specifically for training deep neural network. The model is trained and evaluated over 10 epochs. An epoch is a hyperparameter in the model that represents the entire dataset undergoing a forward and backward propagation through the neural network only once. On each epoch the data is trained on the training set and the training loss and training accuracy is recorded. In addition, on each epoch the model is evaluated on the testing set and the testing loss and testing accuracy is recorded. 

## Results 
The following section will outline some of the notable findings from the training and evaluation step. As mentioned previously, several different LSTM models with different variations of hyperparemetes will be trained and evaluated on a set of both training and testing data over the course of 10 epochs. Although the model is minimizing sparse categorical cross entropy, the accuracy of the classifier is being tracked because it is easily interpretable. There is an important distinction between the training accuracy and testing accuracy of the model. The testing accuracy is the more important metric to track because it provides an estimate on how the model performance will generalize outside of the data it has been trained on. An increase in the training accuracy will increase the testing accuracy to a certain extent, until the model overfits to the training data and the testing accuracy because to decline. As this is the case, it is important to only train a production model to the point where its validation accuracy is highest to ensure the best performance possible. With this in mind, below is the epoch_loss and epoch_accuracy graph generated by TensorBoard for the model with the highest test accuracy score: 

<img width="1050" alt="Screen Shot 2020-01-08 at 2 27 44 PM" src="https://user-images.githubusercontent.com/34798787/72009488-03ab7080-3224-11ea-8119-1bfecbae92bf.png">

<img width="1044" alt="Screen Shot 2020-01-08 at 2 28 16 PM" src="https://user-images.githubusercontent.com/34798787/72009522-14f47d00-3224-11ea-9561-4e9784e32d93.png">

The same information but in a more detailed format generated by TensorFlow logs: 

<img width="1179" alt="Screen Shot 2020-01-08 at 2 39 00 PM" src="https://user-images.githubusercontent.com/34798787/72009859-ba0f5580-3224-11ea-97bf-236c58fd737a.png">

## Resources
https://pythonprogramming.net/recurrent-neural-network-deep-learning-python-tensorflow-keras/

https://blog.patricktriest.com/analyzing-cryptocurrencies-python/

https://towardsdatascience.com/aifortrading-2edd6fac689d

